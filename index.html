<html>

    <body>
    
<h3>crreated by :mohamed magdy fetoh</h3>
<h3>section :36</h3>
<h3>B.N:1</h3>
<h3>groub:6</h3>
<h2>links</h2>
<ul>
<li><a href="technologies.html">technologies</a></li>
<li><a href="Applications.html">Applications</a></li>
<li><a href="criticism.html">criticism</a></li>
<li><a href="architecture.html">architecture</a></li>
</ul>
<h1> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Big Data</h1>
<h2>Brief:</h2>
<p>Challenges include capture , duration, storage  , search, share, transfer, analysis and visualization.  The trend is due to large data sets due to additional information derived from the analysis of one large set of related data, as compared to smaller discrete sets with the same total volume of data, allowing for correlations that reveal “pivotal business trends, determine research quality, and link Legal citations, crime fighting and real-time traffic determination  . 

    As of 2012, limits on the size of data sets suitable for processing in a reasonable amount of time were subject to exabytes .  Scientists usually face a number of limitations due to the large data sets that exist in many fields, which include meteorology ( weather science ), genetics ( genomics )  , and physical simulation.  Complex biological and environmental research,  limitations also affect Internet search ( search engine ), business technology and funding.. Data sets are growing in size, in part, because they are increasingly being collected by mobile information sensors, atmospheric ( remote sensing ) technologies  , program records, cameras, microphones, and frequency transmitters ( determining Identity using radio waves ) and wireless sensor networks . Global technology capacity to store information per capita nearly doubled every 40 months from the 1980s, and as of 2012, it was generating 2.5 quintillion bytes (2.5 x 10 18 ) of data per day.  The challenge for large companies is to determine who should own big data initiatives that will spread across the entire organization. 
    
    It is difficult to work with big data using most RDBMS, desktop statistics and simulation packages, as it instead requires "large-scale parallel software running on tens, hundreds, or even thousands of servers".  What is considered "big data" varies according to the capabilities of the organization managing the collection, and to the capabilities of applications that are traditionally used to process and analyze the data set in its own domain. “For some organizations, encountering hundreds of gigabytes of data for the first time may lead to a reconsideration of data management options. For others, it may take tens or hundreds of terabytes of data before data volume becomes a concern.” </p>

<h2>Definition:</h2>
<p>Big data usually includes data sets of sizes that exceed the capacity of commonly used programs to capture, manage, and process data within an acceptable period of time.  Big data volumes are an ever-moving target, as of 2012, ranging in size from a few tens of terabytes to many petabytes of data in just one set. With this difficulty, new platforms of "big data" tools are being developed to deal with various aspects of large amounts of data.

    In a 2001 research report and related lectures,  META Group (now Gartner) analyst Doug Laney defined the challenges and opportunities of data growth as a three-dimensional element, meaning increasing volume (amount of data), velocity (speed of data) incoming and outgoing) and diversity (the diversity of data types and sources). Gartner and many companies in the industry now continue to use the "3Vs" model to describe big data.  In 2012, Gartner updated its definition to read: "Big data is a large-volume, high-speed, and/or high-diversity information asset that requires new forms of processing to enhance decision-making, deep understanding, and process improvement."
    
    TBDI Definition of Big Data: Big Data is a term applied to large objects of data that vary in nature whether structured, unstructured or semi-structured, including from internal or external sources of the organization, are generated at a high degree of speed with a disordered model, and which do not They are fully aligned with traditional and structured data warehouses and require a robust and complex ecosystem with a high-performance computing platform and analytics capabilities to capture, process, transform, detect, extract value and deep insights within an acceptable time frame.” </p>
<h2>Some images describe big data</h2>
<img src="image 1.png" >
<img src="image 2.png" >
<img src="image 3.png" >

</body>

</html>
